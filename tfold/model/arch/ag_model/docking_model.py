# -*- coding: utf-8 -*-
# Copyright (c) 2023, Tencent Inc. All rights reserved.
import torch
from torch import nn

from tfold.model.layer import Linear, ChainRelativePositionEmbedding, MultimerPositionEmebedding
from tfold.model.module import RecyclingEmbedding, PairPredictor, StructureModule, EvoformerStackSS
from tfold.protein.prot_constants import RESD_NAMES_1C, RESD_WITH_X
from tfold.protein.utils import get_complex_id
from .mono_to_multimer import MonoToMultimer


class DockingModel(nn.Module):
    """The ligand & receptor complex docking model.

    Inputs (at least one is provided)
    * ligand: ligand's amino-acid sequence and feature generated by PLM-based model
    * receptor: receptor's amino-acid sequence and feature generated by MSA-based model
    * complex_id: sequence of complex

    Workflow:
     * initialize inter-chain pair features
     * (s_abi, p_abi, s_abi, p_agi) => (s_c, p_c) # c: ligand-receptor complex

    Args:
        ligand_c_s: number of dimensions in single features (D_s)
        ligand_c_z: number of dimensions in pair features (D_p)
        receptor_c_s: number of dimensions in initial single features for receptor
        receptor_c_z: number of dimensions in initial pair features for receptor
        num_2d_layers: number layers of evoformer
        num_3d_layers: number blocks of structure module
    """

    def __init__(
            self,
            ligand_c_s,
            ligand_c_z,
            receptor_c_s,
            receptor_c_z,
            num_2d_layers=16,
            num_3d_layers=8,
            num_recycles=1,
            num_2d_recycles=1,
            num_3d_recycles=1,
            use_icf=False
    ):
        super().__init__()
        self.ligand_c_s = ligand_c_s
        self.ligand_c_z = ligand_c_z
        self.receptor_c_s = receptor_c_s
        self.receptor_c_z = receptor_c_z
        self.num_2d_layers = num_2d_layers
        self.num_3d_layers = num_3d_layers
        self.num_recycles = num_recycles
        self.num_2d_recycles = num_2d_recycles
        self.num_3d_recycles = num_3d_recycles
        self.use_icf = use_icf
        self.posi_encoder = MultimerPositionEmebedding(64)
        self.crpe_encoder = ChainRelativePositionEmbedding(self.ligand_c_z)
        self.net = nn.ModuleDict()

        # build module for complex structure prediction
        self.net['mono2mult'] = MonoToMultimer(
            ligand_c_s=self.ligand_c_s,
            ligand_c_z=self.ligand_c_z,
            receptor_c_s=self.receptor_c_s,
            receptor_c_z=self.receptor_c_z,
            use_icf=use_icf
        )
        self.net['evoformer'] = EvoformerStackSS(
            num_layers=self.num_2d_layers,
            c_s=self.ligand_c_s,
            c_z=self.ligand_c_z,
        )
        self.net['aa_pred'] = Linear(self.ligand_c_s, len(RESD_NAMES_1C))
        self.net['af2_smod'] = StructureModule(
            num_layers=self.num_3d_layers,
            c_s=self.ligand_c_s,
            c_z=self.ligand_c_z,
            n_dims_encd=64,
            tmsc_pred=True,
        )
        self.net['rc_embed'] = RecyclingEmbedding(
            c_m=self.ligand_c_s,
            c_z=self.ligand_c_z,
        )

        # auxiliary predictions for inter-residue geometries
        self.net['da_pred'] = PairPredictor(
            self.ligand_c_z,
            bins=[37, 25, 25, 25],
            activation='relu_squared'
        )

    def forward(self,
                inputs,
                num_recycles=None,
                num_2d_recycles=None,
                num_3d_recycles=None,
                chunk_size=None):
        """
        Args:
            inputs: dict of input tensors
            num_recycles: number of global (2D + 3D) recycling iterations
            num_2d_recycles: number of 2D-only recycling iterations (EvoformerBlockSS)
            num_3d_recycles: number of 3D-only recycling iterations (AF2SMod)
            chunk_size: chunk_size for inference

        Returns:
            outputs: dict of model predictions

        Notes:
        * The input dict is organized as below (unused data entries are omitted here):
          > ligand:
            > base:
              > seq: ligand's amino-acid sequence of length L1
            > feat:
              > sfea: initial single features of size 1 x L1 x D_s
              > pfea: initial pair features of size 1 x L1 x L1 x D_p
              > cord: initial coordinate features of size 1 x L1 x 14
          > receptor:
            > base:
              > seq: receptor's amino-acid sequence of length L2
            > feat:
              > mfea: initial MSA features of size 1 x M x L2 x D_s
              > pfea: initial pair features of size 1 x L2 x L2 x D_p
              > cord: initial coordinate features of size 1 x L2 x 14
         > ligand:receptor:
            > base:
              > seq: complex's amino-acid sequence of length Lc = (L1 + L2) (no linker)
            > asym_id: asymmetric ID of length Lc
            > feat:

          """
        num_recycles = num_recycles or self.num_recycles
        num_2d_recycles = num_2d_recycles or self.num_2d_recycles
        num_3d_recycles = num_3d_recycles or self.num_3d_recycles
        ligand_id, receptor_id = get_complex_id(inputs)
        complex_id = ':'.join([ligand_id, receptor_id])
        n_resds_list = [len(inputs[x]['base']['seq']) for x in [ligand_id, receptor_id]]
        device = inputs[ligand_id]['feat']['sfea'].device

        # get positional encodings
        chn_infos = list(zip([ligand_id, receptor_id], n_resds_list))
        penc_tns = self.posi_encoder(chn_infos).unsqueeze(dim=0).to(device)
        inputs[complex_id]['base']['chn_infos'] = chn_infos
        inputs[complex_id]['feat']['penc'] = penc_tns
        asym_id = inputs[complex_id]['asym_id'][0]

        outputs = {}
        rc_inputs = None

        # get ligand initial features
        ligand_feat = inputs[ligand_id]['feat']
        ligand_feat['seq'] = inputs[ligand_id]['base']['seq']
        if 'mask' in inputs[ligand_id]['base']:
            mask = inputs[ligand_id]['base']['mask']
            modified_seq = ''.join(['G' if mask[i] == 1 else ligand_feat['seq'][i]
                                    for i in range(len(ligand_feat['seq']))])
            ligand_feat['seq'] = modified_seq

        # get receptor initial features
        receptor_feat = {
            'seq': inputs[receptor_id]['base']['seq'],
            'mfea': inputs[receptor_id]['feat']['mfea'][-1].unsqueeze(dim=0).to(device).to(ligand_feat['sfea'].dtype),
            'pfea': inputs[receptor_id]['feat']['pfea'][-1].unsqueeze(dim=0).to(device).to(ligand_feat['pfea'].dtype),
            'cord': inputs[receptor_id]['feat']['cord'][-1].to(device).to(ligand_feat['cord'].dtype),
        }

        # update single & pair features w/ inter-chain features
        if self.use_icf and inputs[complex_id]['feat']['icf'] is not None:
            ic_feat = inputs[complex_id]['feat']['icf'].to(device)
        else:
            ic_feat = None

        # get initial single & pair features
        sfea_tns, pfea_tns = self.net['mono2mult'](ligand_feat, receptor_feat, ic_feat)

        # update pair features w/ chain relative positional encodings
        pfea_tns += self.crpe_encoder(chn_infos, asym_id)

        for cycle_idx in range(num_recycles):
            # disable gradient computation except for the last recycle
            requires_grad = (self.training and (cycle_idx == num_recycles - 1))
            with torch.set_grad_enabled(requires_grad):
                self.net['evoformer'].requires_grad_(requires_grad)

                # RcEmbedNet
                if rc_inputs is not None:
                    sfea_tns_ext, pfea_tns = self.net['rc_embed'](
                        inputs[complex_id]['base']['seq'], sfea_tns.unsqueeze(dim=1), pfea_tns, rc_inputs)
                    sfea_tns = sfea_tns_ext.squeeze(dim=1)

                # Evoformer
                sfea_tns, pfea_tns = self.net['evoformer'](sfea_tns, pfea_tns,
                                                           num_recycles=num_2d_recycles,
                                                           chunk_size=chunk_size)
                # MLM prediction head
                if 'mask' in inputs[ligand_id]['base']:
                    logt_tns_aa = self.net['aa_pred'](sfea_tns)
                    logt_tns_aa = logt_tns_aa.permute(0, 2, 1)  # move classification logits to the 2nd dimensions
                    pred_token = torch.argmax(logt_tns_aa, dim=1)
                    inpt_token = torch.LongTensor(
                        [RESD_WITH_X.index(x) for x in inputs[ligand_id]['base']['seq']]).to(device)
                    output_token = torch.where(
                        inputs[ligand_id]['base']['mask'] == 1,
                        pred_token[0][:len(inputs[ligand_id]['base']['seq'])],
                        inpt_token)
                    ligand_seq = ''.join([RESD_NAMES_1C[x] for x in output_token])
                    # update the complex sequence
                    inputs[complex_id]['base']['seq'] = ligand_seq + inputs[receptor_id]['base']['seq']

                # inter-residue distance & angle predictions
                logt_tns_cb, logt_tns_om, logt_tns_th, logt_tns_ph = self.net['da_pred'](pfea_tns)

                # AF2SMod
                params_list, plddt_list, cord_list, fram_tns_sc, tmsc_dict = self.net['af2_smod'](
                    inputs[complex_id]['base']['seq'], sfea_tns, pfea_tns, penc_tns, num_3d_recycles,
                    asym_id=asym_id,
                )

                # pack all the outputs into a dict
                outputs = {
                    'sfea': sfea_tns,
                    'pfea': pfea_tns,
                    'penc': penc_tns,
                    '1d': {},
                    '2d': {
                        'cb': logt_tns_cb,
                        'om': logt_tns_om,
                        'th': logt_tns_th,
                        'ph': logt_tns_ph,
                    },
                    '3d': {
                        'params': params_list,
                        'plddt': plddt_list,
                        'cord': cord_list,
                        'fram_sc': fram_tns_sc,
                        'tmsc_dict': tmsc_dict,
                    },
                }
                if 'mask' in inputs[ligand_id]['base']:
                    outputs['1d']['pred_tns'] = logt_tns_aa
                    outputs['1d']['seq'] = ligand_seq

                # collect recycling inputs
                rc_inputs = {
                    'sfea': outputs['sfea'].detach(),
                    'pfea': outputs['pfea'].detach(),
                    'cord': outputs['3d']['cord'][-1].detach(),
                }

        return outputs
